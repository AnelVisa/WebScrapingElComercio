{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x8PpSEv3lpk",
        "outputId": "3b7e0a61-7c2c-4b32-d137-706df4af9e16"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "# URL de la página web que deseamos hacer scraping\n",
        "url = 'https://elcomercio.pe/archivo/todas/2023-10-02/'\n",
        "\n",
        "# Realizamos una solicitud GET a la página web\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificamos si la solicitud fue exitosa (código de respuesta 200)\n",
        "if response.status_code == 200:\n",
        "    # Parseamos el contenido HTML de la página web\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Encontramos todos los elementos con la etiqueta 'span' que contienen la fecha y hora\n",
        "    Fecha_Hora = soup.find_all('p', class_=\"story-item__date font-thin ml-5 text-xs text-gray-300 md:mt-5 md:ml-0\")\n",
        "\n",
        "    # Encontramos todos los elementos con la etiqueta 'h2' que contienen los títulos de las noticias\n",
        "    titulos = soup.find_all('h2')\n",
        "\n",
        "    # Encontramos todos los elementos con la etiqueta 'div' que contienen el autor\n",
        "    Author = soup.find_all('div', class_=\"hidden\")\n",
        "\n",
        "    # Crear DataFrames para cada categoría\n",
        "    df_fecha_hora = pd.DataFrame({'Fecha_Hora': [fechaH.text.strip() for fechaH in Fecha_Hora]})\n",
        "    df_titulos = pd.DataFrame({'Títulos': [titulo.text.strip() for titulo in titulos]})\n",
        "    df_author = pd.DataFrame({'Autor': [Authors.text.strip() for Authors in Author]})\n",
        "\n",
        "    # Combina los DataFrames\n",
        "    df = pd.concat([df_fecha_hora, df_titulos, df_author], axis=1)\n",
        "\n",
        "    # Agregar una columna de numeración a los registros\n",
        "    df.insert(0, 'Número', range(1, len(df) + 1))\n",
        "\n",
        "    # Imprimir el DataFrame de forma ordenada en una tabla con bordes y numeración\n",
        "    print(tabulate(df, headers='keys', tablefmt='grid', showindex=False, numalign='left'))\n",
        "\n",
        "    # Crear un nuevo archivo Excel\n",
        "    nombre_archivo = 'web_scraping_elcomercio_2023-10-02.xlsx'\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "    ws.title = 'Resultados'\n",
        "\n",
        "    # Agregar los datos del DataFrame al archivo Excel\n",
        "    for row in dataframe_to_rows(df, index=False, header=True):\n",
        "        ws.append(row)\n",
        "\n",
        "    # Guardar el archivo Excel\n",
        "    wb.save(nombre_archivo)\n",
        "\n",
        "    print(f'Se han guardado los resultados en \"{nombre_archivo}\" con formato de tabla.')\n",
        "else:\n",
        "    print('Error al obtener la página:', response.status_code)\n"
      ],
      "metadata": {
        "id": "LXPr6e3D3lkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6XTu07fB2st"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KbtxxBTvUc5U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}